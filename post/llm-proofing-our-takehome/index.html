<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.125.7"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>LLM Proofing Our Takehome Challenge&nbsp;&ndash;&nbsp;Philosophical Hacker</title><link rel="stylesheet" href="/css/core.min.63f706677e61b4ee62b8daf083358d3bbf8ac8ab03c7d171af3180fab3a3ebb83efb79fb98674f13dde6db11de2bf694.css" integrity="sha384-Y/cGZ35htO5iuNrwgzWNO7&#43;KyKsDx9FxrzGA&#43;rOj67g&#43;&#43;3n7mGdPE93m2xHeK/aU"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="LLM Proofing Our Takehome Challenge" />
<script type="text/javascript">
  window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=document.createElement("script");r.type="text/javascript",r.async=!0,r.src="https://cdn.heapanalytics.com/js/heap-"+e+".js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(r,a);for(var n=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","resetIdentity","removeEventProperty","setEventProperties","track","unsetEventProperty"],o=0;o<p.length;o++)heap[p[o]]=n(p[o])};
  heap.load("2872977926");
</script>
<script type="application/javascript">
  (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="Xy0yMDEzNTMyMTI3";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/Xy0yMDEzNTMyMTI3/oribi.js","ORIBI");
</script>


<link rel="canonical" href="https://makefizz.buzz/posts/llm-proof"/>
<body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name">Philosophical Hacker</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/talk">Talks</a><a class="nav item" href="https://www%2eoreilly%2ecom/programming/free/rxjava-for-android-app-development%2ecsp?intcmp=il-prog-free-product-na_new_site_rxjava_for_android_app_development"target="_blank" rel="noopener noreferrer">RxJava O&#39;Reilly Book</a><a class="nav item" href="/note">Notes</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">LLM Proofing Our Takehome Challenge</h1><p class="article date">2025-06-20<span class="reading-time"> â€¢ 2 minutes to read</span></p></section><article class="article markdown-body"><p>Our original idea for our coding challenge was to ask candidates to build a tic-tac-toe game in React with a few curve balls thrown in around making the solution more general for larger game boards and play modes. We scrapped that idea when we discovered ChatGPT could trivially do this.</p>
<p>Here are some things we did to &ldquo;LLM-proof&rdquo; our new challenge.</p>
<p>But first, why the scare quotes around &ldquo;LLM-proof.&rdquo; Because I&rsquo;m sure someone could get an LLM to solve the new challenge, but the quality of prompting that would be necessary to pull this off would be an indication of a strong engineer. The point of an interview process is to find strong engineers, not to find ones who don&rsquo;t use LLMs.</p>
<p>First, we focused the challenge on a product-specific feature instead of a toy problem. Products need to be differentiated to exist at all, so starting the challenge w/ a unique product feature already sets us up well for finding a task that&rsquo;s different from what was in the LLM&rsquo;s training dataset.</p>
<p>Second, we made abstraction and good design a core part of the challenge. Again, this is something that LLMs aren&rsquo;t particularly great at. This shouldn&rsquo;t be surprising since most code that LLMs are trained on is a mess.</p>
<p>Third, we asked candidates to post a 30 minute video of them solving the challenge. This helped us get a sense of how much they were leaning on an LLM while they were solving the challenge. If it looked like they didn&rsquo;t understand basic language concepts, we didn&rsquo;t move forward with them.</p>
<p>Finally, we playtested the challenge with LLM assistance. The LLM was able to come up with a basic implementation with one or two prompts. However, trying to get the LLM to refactor its code to be more flexible was a lot harder. We gave up after 90 minutes of trying.</p>
<p>Without giving away too much of the challenge, all we wanted was the LLM to introduce a new Typescript inteface that would DRY up the React code it generated and enable additional implementations of the interface. I couldn&rsquo;t get ChatGPT 4o to do it.</p>
<p>If some LLM whisperer could get an LLM make the refactor, great. Welcome to the team, but for the dozens of solutions seen since implementing the new challenge, the hires we&rsquo;ve made wrote the solution the old fashioned way.</p>
</article>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/post/libraries-llms/"><span class="iconfont icon-article"></span>Libraries are under-used. LLMs make this problem worse.</a></p><p><a class="link" href="/post/value-based-pricing-is-a-trap/"><span class="iconfont icon-article"></span>Value-based pricing can be a trap for early startups</a></p></section></div></section><section id="footer"><div style="display: flex; flex-direction: row; justify-content: center;">
  <p>
    &copy; 2025 Matt Dupree
  </p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script><script src="/js/core.min.38b060a751ac96384cd9327eb1b1e36a21fdb71114be07434c0cc7bf63f6e1da274edebfe76f65fbd51ad2f14898b95b.js" integrity="sha384-OLBgp1GsljhM2TJ&#43;sbHjaiH9txEUvgdDTAzHv2P24donTt6/529l&#43;9Ua0vFImLlb"></script></body>

</html>