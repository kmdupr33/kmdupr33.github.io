<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.125.7"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>An Intro to Gradient Descent for Kotlin Programmers&nbsp;&ndash;&nbsp;Philosophical Hacker</title><link rel="stylesheet" href="/css/core.min.63f706677e61b4ee62b8daf083358d3bbf8ac8ab03c7d171af3180fab3a3ebb83efb79fb98674f13dde6db11de2bf694.css" integrity="sha384-Y/cGZ35htO5iuNrwgzWNO7&#43;KyKsDx9FxrzGA&#43;rOj67g&#43;&#43;3n7mGdPE93m2xHeK/aU"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="An Intro to Gradient Descent for Kotlin Programmers" />
<script type="text/javascript">
  window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=document.createElement("script");r.type="text/javascript",r.async=!0,r.src="https://cdn.heapanalytics.com/js/heap-"+e+".js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(r,a);for(var n=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","resetIdentity","removeEventProperty","setEventProperties","track","unsetEventProperty"],o=0;o<p.length;o++)heap[p[o]]=n(p[o])};
  heap.load("2872977926");
</script>
<script type="application/javascript">
  (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="Xy0yMDEzNTMyMTI3";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/Xy0yMDEzNTMyMTI3/oribi.js","ORIBI");
</script>

<body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name">Philosophical Hacker</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/talk">Talks</a><a class="nav item" href="https://www%2eoreilly%2ecom/programming/free/rxjava-for-android-app-development%2ecsp?intcmp=il-prog-free-product-na_new_site_rxjava_for_android_app_development"target="_blank" rel="noopener noreferrer">RxJava O&#39;Reilly Book</a><a class="nav item" href="/note">Notes</a></nav></div></span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">An Intro to Gradient Descent for Kotlin Programmers</h1><p class="article date">2019-09-01</p></section><article class="article markdown-body"><script src="https://cdn.plot.ly/plotly-1.2.0.min.js"></script>
<h2 id="introduction">Introduction</h2>
<p>Gradient descent is an algorithm that&rsquo;s used to solve supervised learning and deep learning problems. Here I&rsquo;m going to try to give you an idea of why the algorithm works and how you&rsquo;d implement it in Kotlin. I&rsquo;ll also show the algorithm working with <a href="https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings"target="_blank" rel="noopener noreferrer">a simple kaggle dataset involving video game sales and ratings</a>.</p>
<p>Everything I cover here is covered in <a href="https://www.coursera.org/learn/machine-learning"target="_blank" rel="noopener noreferrer">Andrew Ng&rsquo;s excellent Coursera machine learning course</a> with the exception of the Kotlin implementation of gradient descent. If you <em>really</em> want a clear and definitive introduction to gradient descent, I recommend that course over this article. Here I&rsquo;m mostly interested in solidifying what I&rsquo;ve learned by sharing it.</p>
<h2 id="our-toy-problem">Our Toy Problem</h2>


<div id="videogameSales"></div>
<script type="module">
  import {scatter, layout} from "./index.js";      
  const data = [scatter];
  Plotly.newPlot('videogameSales', data, layout, {staticChart: true, responsive: true});
</script>

<p>Here&rsquo;s a plot of a bunch of video games from 2010 to 2016. On the X axis, we have a meta-critic score. On the y-axis, we see the number of sales in North America in millions. Looking at the data, it looks like there is some sort of relationship between meta-critic score and game sales.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> Our toy problem is this: we want to write a program that can give a prediction of video game sales based on metric critic score.</p>
<p>If we were looking at this in excel or google sheets, we&rsquo;d just tick the &ldquo;draw trendline&rdquo; checkbox on our chart options and be almost done with our machine learning problem. Excel is fine for some supervised learning tasks, but it won&rsquo;t work for harder problems, which is where gradient descent becomes essential.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Although gradient descent is probably overkill for this toy problem, we&rsquo;re going to use it anyway since presumably whatever boring learning algorithm that&rsquo;s been sitting in excel for decades can&rsquo;t scale to more interesting contemporary ML problems.</p>
<p>Anyway, if we drew the line of best fit for the above data, it&rsquo;d look like this:</p>


<div id="games-with-line"></div>
<script type="module">
  import {scatter, layout, lineOfBestFit} from "./index.js";      
  const data = [scatter, lineOfBestFit];
  Plotly.newPlot('games-with-line', data, layout, {staticChart: true, responsive: true});
</script>

<p>If you remember your high-school algebra, equations for lines like this look like this:</p>


<div id="linear-equation-formula"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("y = mx \u002b b", document.getElementById("linear-equation-formula"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>

<p>So, our problem here is really this: How do we figure out what <code>m</code> and <code>b</code> are given the above video game data? Or, how would we implement the following kotlin function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">fun</span> <span style="color:#a6e22e">findLearningParameters</span>(videoGameData: Array&lt;Pair&lt;Int, Int&gt;&gt;): Pair&lt;Int, Int&gt; {
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">//...
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">return</span> Pair(m, b)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This is really the crux of the problem because once we this function implemented, it&rsquo;s trivial to write a program that can predict video game sales based on meta-critic score:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">fun</span> <span style="color:#a6e22e">predictVideoGameSales</span>(
</span></span><span style="display:flex;"><span>  metaCriticScore: Int,
</span></span><span style="display:flex;"><span>  learningParameters: Pair&lt;Int, Int&gt;
</span></span><span style="display:flex;"><span>) {
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>m, b) = learningParameters
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> metaCriticScore * m + b
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>What we want is for the learning parameters to be such that the difference between the predicted values (given by the line of best fit) and all the actual values is minimized. This difference <em>depends</em> on the values we choose for <code>x</code> and <code>b</code>; it&rsquo;s a function of those variables. This function is called the &ldquo;cost function,&rdquo; so another way of putting our problem is to say that we&rsquo;re trying to find values of <code>x</code> and <code>b</code> that minimize the cost function.</p>
<h2 id="solution">Solution</h2>
<p>To start understanding how we&rsquo;d solve this problem, we need to take a closer look at the cost function. What exactly is it? Let&rsquo;s start with a thought experiment: suppose we choose 0 as the values for <code>x</code> and <code>b</code>. What would the difference be between the <em>actual values</em> from our data set and the predicted values? The following table with the first five data points tries to capture this:</p>
<table>
<thead>
<tr>
<th>meta-critic score</th>
<th>actual sales</th>
<th>predicted sales</th>
<th>(predicted - actual)^2</th>
</tr>
</thead>
<tbody>
<tr>
<td>61</td>
<td>15</td>
<td>0</td>
<td>225</td>
</tr>
<tr>
<td>97</td>
<td>7.02</td>
<td>0</td>
<td>49</td>
</tr>
<tr>
<td>97</td>
<td>9.66</td>
<td>0</td>
<td>93.3</td>
</tr>
<tr>
<td>88</td>
<td>9.04</td>
<td>0</td>
<td>81.7216</td>
</tr>
<tr>
<td>87</td>
<td>9.7</td>
<td>0</td>
<td>94.09</td>
</tr>
</tbody>
</table>
<p>One way to summarize this table would be to take the average of the squared difference between the predicted value and the actual value and divide by 2. That&rsquo;s our cost function, which we&rsquo;ll call <code>J</code>. Formally, that&rsquo;s:</p>


<div id="cost-function"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("J(m, b) = \\dfrac{1}{2} \\sum_{k=1}^n ((mx \u002b b) - actual)^2 \/ n", document.getElementById("cost-function"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>

<p>where n is the number of video games in our data set. (If we were to use this equation on the above table, we&rsquo;d get 54.31.) Let&rsquo;s ignore <code>b</code> for a second and rewrite our equation:</p>


<div id="cost-function-ignoring-b"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("J(m) = \\dfrac{1}{2} \\sum_{k=1}^n (mx - actual)^2 \/ n", document.getElementById("cost-function-ignoring-b"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>

<p>Remember: <code>x</code> and <code>actual</code> aren&rsquo;t variables here, we&rsquo;ll be able to plug in values from our data, so we&rsquo;re really just looking at a quadratic function and you probably remember from high-school that quadratic functions look like this:</p>


<div id="quadratic-equations"></div>
<script type="module">
  const xs = [-5 ,-4, -3, -2, -1, 0, 1, 2, 3, 4, 5];
  const data = [{
    x: xs,
    y: xs.map(x => x * x),    
    line: {
      shape: 'spline'
    }
  }]
  Plotly.newPlot('quadratic-equations', data, 
    { 
      xaxis: {
        title: 'm'
      },
      yaxis: {
        title: 'J(m)'
      }
    }, 
  {staticChart: true, responsive: true});
</script>

<p>The x-axis in this case is <code>m</code> and the y axis tells us how far off <code>m</code> is from where it needs to be. This graph tells us that <code>0</code> is our best choice for <code>m</code> since choosing <code>0</code> will actually minimize the cost function. This isn&rsquo;t true of our actual data set, but the basic idea of gradient descent applies regardless and this graph is (probably) cleaner to look at.</p>
<p>At the highest level, the way gradient descent works is that we take a guess at the optimal value of <code>m</code>, then we modify our guess by a small amount called the &ldquo;learning rate&rdquo; and repeat until we aren&rsquo;t getting any better. This is how the algorithm got its name: we&rsquo;re slowing descending down the gradient until we get to the minimum.</p>
<p>With that, we can start to fill in more of our <code>findLearningParameters</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">fun</span> <span style="color:#a6e22e">findLearningParameters</span>(videoGameData: Array&lt;Pair&lt;Double, Double&gt;&gt;): Pair&lt;Double, Double&gt; {
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">val</span> learningRate = .<span style="color:#ae81ff">0003</span>
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">var</span> guess = Pair(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">for</span> (i <span style="color:#66d9ef">in</span> <span style="color:#ae81ff">1.</span>.<span style="color:#ae81ff">10000000</span>) {
</span></span><span style="display:flex; background-color:#3c3d38"><span>     guess = updateGuess(guess, learningRate, videoGameData)
</span></span><span style="display:flex;"><span>     <span style="color:#66d9ef">if</span> (i % <span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) println(<span style="color:#e6db74">&#34;Guess: </span><span style="color:#e6db74">$guess</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex; background-color:#3c3d38"><span>        <span style="color:#66d9ef">if</span> (i % <span style="color:#ae81ff">1000</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) println(<span style="color:#e6db74">&#34;Cost: </span><span style="color:#e6db74">${cost(videoGameData, guess)}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>     }
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">return</span> guess
</span></span><span style="display:flex;"><span>}</span></span></code></pre></div>
<p>The highlighted lines point to some functions that need explaining. You should be able to guess at what <code>cost</code> does: it computes how bad our guess at <code>m</code> and <code>b</code> are with the aforementioned formula:</p>


<div id="cost-function-2"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("J(m, b) = \\sum_{k=1}^n ((mx \u002b b) - actual)^2 \/ n", document.getElementById("cost-function-2"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>

<p>So, the implementation is just this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">fun</span> <span style="color:#a6e22e">cost</span>(videoGameData: Array&lt;Pair&lt;Double, Double&gt;&gt;, learningParameters: Pair&lt;Double, Double&gt;): Double =
</span></span><span style="display:flex;"><span>        videoGameData.fold(<span style="color:#ae81ff">0.0</span>) { acc: Double, pair: Pair&lt;Double, Double&gt; <span style="color:#f92672">-&gt;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>m, b) = learningParameters
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>x, y) = pair
</span></span><span style="display:flex;"><span>            acc + ((((m * x) + b) - y).pow(<span style="color:#ae81ff">2</span>)/<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        } / videoGameData.size
</span></span></code></pre></div><p>The reason we are using the cost function is to monitor progress on the gradient descent. As the for-loop executes, <code>cost</code> should be going down.</p>
<p>Understanding the implementation of <code>updateGuess</code> requires a little detour into basic calculus.</p>
<p>If we look at our graph of <code>m</code> and <code>J(m)</code> and remember that the derivative of a function gives us the slope of the tangent line, we have what we need to see the central insight that makes gradient descent possible. So, let&rsquo;s ask: What happens if we compute the derivative of this function and draw the tangent lines for <code>m = 1</code> and <code>m=-1</code> in this graph?</p>


<div id="quadratic-equations-with-deriv"></div>
<script type="module">
  const xs = [-5 ,-4, -3, -2, -1, 0, 1, 2, 3, 4, 5];
  const data = [{
    x: xs,
    y: xs.map(x => x * x),    
    name: 'J(m)',
    line: {
      shape: 'spline'
    }
  }, {
    name: 'tangent line m=1',
    x: [-2, -1, 0, 1, 2],
    y: [-5, -3, -1, 1, 3]
  },
  {
    name: 'tangent line m=-1',
    x: [-2, -1, 0, 1, 2],
    y: [3, 1, -1, -3, -5]
  }]
  Plotly.newPlot('quadratic-equations-with-deriv', data, 
    { 
      xaxis: {
        title: 'm'
      },
      yaxis: {
        title: 'J(m)'
      }
    }, 
    {staticChart: true, responsive: true}
  );
</script>

<p>The tangent line for m=1 has a <em>positive</em> slope, which means the derivative of <code>J(m)</code> at that point will be positive. The tangent line for m=-1 has a <em>negative</em> slope, which means the derivative of <code>J(m)</code> at that point will be negative. If m=1, we want our next guess at <code>m</code> to be less and if m=-1, we want our next guess to be more. We can easily achieve this by subtracting the derivative, and if we do this, we wind up with the heart of the gradient descent algorithm:</p>
<p>

<div id="update-rule-m"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("m\u0027 = m - \\alpha \\frac{\\partial J}{\\partial m}J(m,b)", document.getElementById("update-rule-m"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>



<div id="update-rule-b"></div>
<script>
  addEventListener("DOMContentLoaded", () => {
    katex.render("b\u0027 = b - \\alpha \\frac{\\partial J}{\\partial b}J(m,b)", document.getElementById("update-rule-b"), {
      throwOnError: false,
      displayMode: true
    });
  });
</script>
</p>
<p>We&rsquo;ve got some partial derivatives here because we&rsquo;re working with multiple variables, and in general, there may be many more learning parameters than <code>m</code> and <code>b</code> if we have more features/variables in our learning problem (e.g., we may think that game studio is a predictor/variable that could tell us something about game sales). This detail doesn&rsquo;t really matter for getting the gist of gradient descent. Here&rsquo;s the kotlin:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-kotlin" data-lang="kotlin"><span style="display:flex;"><span><span style="color:#66d9ef">fun</span> <span style="color:#a6e22e">updateGuess</span>(
</span></span><span style="display:flex;"><span>        guess: Pair&lt;Double, Double&gt;,
</span></span><span style="display:flex;"><span>        learningRate: Double,
</span></span><span style="display:flex;"><span>        videoGameData: Array&lt;Pair&lt;Double, Double&gt;&gt;
</span></span><span style="display:flex;"><span>    ): Pair&lt;Double, Double&gt; {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>m, b)  = guess
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">val</span> mGradient = videoGameData.fold(<span style="color:#ae81ff">0.0</span>) { acc: Double, pair: Pair&lt;Double, Double&gt; <span style="color:#f92672">-&gt;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>x, y) = pair
</span></span><span style="display:flex;"><span>            acc + ((((m * x) + b) - y) * x)
</span></span><span style="display:flex;"><span>        } / videoGameData.size
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">val</span> bGradient = videoGameData.fold(<span style="color:#ae81ff">0.0</span>) { acc: Double, pair: Pair&lt;Double, Double&gt; <span style="color:#f92672">-&gt;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">val</span> <span style="color:#960050;background-color:#1e0010">(</span>x, y) = pair
</span></span><span style="display:flex;"><span>            acc + ((((m * x) + b) - y))
</span></span><span style="display:flex;"><span>        } / videoGameData.size
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">val</span> newM = m - (learningRate * mGradient)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">val</span> newB = b - (learningRate * bGradient)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> Pair(newM, newB)
</span></span><span style="display:flex;"><span>    }
</span></span></code></pre></div><p>Running this code gives us <code>.0281</code> for <code>m</code> and <code>-.804</code> for <code>b</code>, which is pretty darn close to that line of best fit we saw earlier.</p>
<h2 id="notes">Notes</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The relationship isn&rsquo;t as strong as I suspected. They say being a game dev is rough. Maybe this is evidence of that. Maybe the data just isn&rsquo;t very good.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>I&rsquo;m betting excel and google sheets use the &ldquo;normal equation&rdquo; method of linear regression rather than gradient descent, which is <a href="https://www.coursera.org/learn/machine-learning/supplement/bjjZW/normal-equation"target="_blank" rel="noopener noreferrer">apparently ok if you&rsquo;re dealing with a linear regression problem with less than 10k features</a>. I also doubt it can handle more than one variable without any sort of extension.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</article><section class="article labels"><a class="tag" href=/tags/machine-learning/>machine learning</a><a class="tag" href=/tags/kotlin/>kotlin</a></section>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/post/maybe-dont-write-that-test/"><span class="iconfont icon-article"></span>Maybe Don't Write That Test</a></p><p><a class="link" href="/post/dagger-2-years-later/"><span class="iconfont icon-article"></span>Dagger 2, 2 Years Later</a></p></section></div></section><section id="footer"><div style="display: flex; flex-direction: row; justify-content: center;">
  <p>
    &copy; 2024 Matt Dupree
  </p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script><script src="/js/core.min.38b060a751ac96384cd9327eb1b1e36a21fdb71114be07434c0cc7bf63f6e1da274edebfe76f65fbd51ad2f14898b95b.js" integrity="sha384-OLBgp1GsljhM2TJ&#43;sbHjaiH9txEUvgdDTAzHv2P24donTt6/529l&#43;9Ua0vFImLlb"></script></body>

</html>