<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.125.7"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>Avatar The Last Airbender Language Model and Text Generation&nbsp;&ndash;&nbsp;Philosophical Hacker</title><link rel="stylesheet" href="/css/core.min.63f706677e61b4ee62b8daf083358d3bbf8ac8ab03c7d171af3180fab3a3ebb83efb79fb98674f13dde6db11de2bf694.css" integrity="sha384-Y/cGZ35htO5iuNrwgzWNO7&#43;KyKsDx9FxrzGA&#43;rOj67g&#43;&#43;3n7mGdPE93m2xHeK/aU"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="Avatar The Last Airbender Language Model and Text Generation" />
<script type="text/javascript">
  window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=document.createElement("script");r.type="text/javascript",r.async=!0,r.src="https://cdn.heapanalytics.com/js/heap-"+e+".js";var a=document.getElementsByTagName("script")[0];a.parentNode.insertBefore(r,a);for(var n=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","resetIdentity","removeEventProperty","setEventProperties","track","unsetEventProperty"],o=0;o<p.length;o++)heap[p[o]]=n(p[o])};
  heap.load("2872977926");
</script>
<script type="application/javascript">
  (function(b,o,n,g,s,r,c){if(b[s])return;b[s]={};b[s].scriptToken="Xy0yMDEzNTMyMTI3";b[s].callsQueue=[];b[s].api=function(){b[s].callsQueue.push(arguments);};r=o.createElement(n);c=o.getElementsByTagName(n)[0];r.async=1;r.src=g;r.id=s+n;c.parentNode.insertBefore(r,c);})(window,document,"script","https://cdn.oribi.io/Xy0yMDEzNTMyMTI3/oribi.js","ORIBI");
</script>

<body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name">Philosophical Hacker</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/talk">Talks</a><a class="nav item" href="https://www%2eoreilly%2ecom/programming/free/rxjava-for-android-app-development%2ecsp?intcmp=il-prog-free-product-na_new_site_rxjava_for_android_app_development"target="_blank" rel="noopener noreferrer">RxJava O&#39;Reilly Book</a><a class="nav item" href="/note">Notes</a></nav></div></span></div></section><section id="content"><style>
  pre {
    background: none;
  }
</style>
<div class="article-container"><section class="article header">
    <h1 class="article title">Avatar The Last Airbender Language Model and Text Generation</h1><p class="article date">2020-08-15</p></section><article class="article markdown-body">
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p><a href="https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-08-11/readme.md">This week’s TidyTuesday data set</a> is basically the script for all of the <em>Avatar: The Last Airbender</em> episodes. I thought it’d be neat to build a language model from the script and use it to generate new sentences that plausibly sound like they belong in the <em>Avatar</em> universe. In this post, I’ll quickly present some of the sentences generated from the model, and then I’ll dive into how I obtained those results.</p>
<div id="results" class="section level1">
<h1>Results</h1>
<p>Here I show what the model generates based on the first 60 characters of the script, which are:</p>
<blockquote>
<p>water. earth. fire. air. my grandmother used to tell me stor</p>
</blockquote>
<p><br/>
All of the following paragraphs start the same, but have different “temperatures” set to affect how “creative” the sequences are. (More on temperature in the next section.) These passages are ordered by increasing “creativity” (“weirdness” might be a better adjective):</p>
<p><br/></p>
<blockquote>
<p>water. earth. fire. air. my grandmother used to tell me storing to the last the way to the statue who is standing up and standing on the statue who is standing on the shot catches the water and the ship. the scene cuts to a shot of the saddle. the camera pans to the statue who stands the statue. the camera pans to the large standing and the large still pull in the water and the scene cuts to a shot of the statue who standing and standing at the shot catche</p>
</blockquote>
<p><br/></p>
<blockquote>
<p>water. earth. fire. air. my grandmother used to tell me storing interrupted by the tribe pun to the fire nation is the camera pans to the tribler the prove on the way to the statues and and the blue spirit whenging the water who is thrown and and the scene switches to a side-view of the ship on the ground, lit and the side of the right before close-up of the statue like the way like and the water. he approaches his back and close for a terchest make and so</p>
</blockquote>
<p><br/></p>
<blockquote>
<p>water. earth. fire. air. my grandmother used to tell me storio his with the strang. pait of the fire nation gainatenes appa’s ready whate a lard off him to save to gladleg, but if unhake. it starts her lind cheer all the way provely. sokka grab. appa revealing moos tides an and path a few curious chance from shoke him. the cruff. his cloti wivelition to the call aang the river, roke vines a mashes tomorement with a hore it toward. the water a and him flase</p>
</blockquote>
<p><br/></p>
<blockquote>
<p>water. earth. fire. air. my grandmother used to tell me stortel over his misting frog honor with the group his uncle twithing understames sokka. [close-up.she midenct in aang, switche. aang’s belaps out other and onmack laufly. follojes, making the leggrously.] ne! wo ohan’s perfort’s a changing.jumps a finors, don’t have a dirk.[glans.]yoo didn’t menn toges up! [she eldsyles exactunting to slowly.] any the times? i don’t be fry to gomp,[working something</p>
</blockquote>
<p><br/></p>
<p>The results here aren’t wonderful. They’re based on a language model with ~60% accuracy. There’s definitely room for improvement. I could train the model longer, play with the hyperparmeters, and try to get more of the data loaded into the model. (As you’ll see, I couldn’t train on the entire dataset because it was too big.) Still, I’m pretty happy with these results, as this is my first go at using the keras to R interface and I spent quite a bit of time getting all of this working on a cloud VM. Hopefully, the next tidy Tuesday deep learning project will have more interesting results.</p>
</div>
<div id="method" class="section level1">
<h1>Method</h1>
<p>I built the model on a Google Cloud VM running RStudio w/ 30GB of RAM and a Nvidia K80 GPU. Training took about 15 minutes. It wasn’t possible to really do this on my macbook. One epoch was going to take 5 minutes. On the VM, it took 30 seconds. Much better for iterating quickly.</p>
<p><br/></p>
<p>First, download the Tidy Tuesday dataset:</p>
<pre class="r"><code>library(tidyverse)
library(stringr)
library(keras)

avatar &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-08-11/avatar.csv&#39;)
glimpse(avatar)</code></pre>
<pre><code>## Rows: 13,385
## Columns: 11
## $ id              &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…
## $ book            &lt;chr&gt; &quot;Water&quot;, &quot;Water&quot;, &quot;Water&quot;, &quot;Water&quot;, &quot;Water&quot;, &quot;Water&quot;,…
## $ book_num        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ chapter         &lt;chr&gt; &quot;The Boy in the Iceberg&quot;, &quot;The Boy in the Iceberg&quot;, &quot;…
## $ chapter_num     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ character       &lt;chr&gt; &quot;Katara&quot;, &quot;Scene Description&quot;, &quot;Sokka&quot;, &quot;Scene Descri…
## $ full_text       &lt;chr&gt; &quot;Water. Earth. Fire. Air. My grandmother used to tell…
## $ character_words &lt;chr&gt; &quot;Water. Earth. Fire. Air. My grandmother used to tell…
## $ writer          &lt;chr&gt; &quot;‎Michael Dante DiMartino, Bryan Konietzko, Aaron Eha…
## $ director        &lt;chr&gt; &quot;Dave Filoni&quot;, &quot;Dave Filoni&quot;, &quot;Dave Filoni&quot;, &quot;Dave Fi…
## $ imdb_rating     &lt;dbl&gt; 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1…</code></pre>
<p>Including all 13k rows leads to out of memory issues when training the model, so we filter to grab the first 6k:</p>
<pre class="r"><code>trunc_avatar &lt;- avatar %&gt;% 
  filter(id &lt; 6000)</code></pre>
<p>Values greater than 6k lead to OOM issues on either the data preparation step or on the model fitting step.</p>
<p><br/></p>
<p><code>full_text</code> is the column we’re interested in, so we collapse the <code>full_text</code> of every row into a single string:</p>
<pre class="r"><code>all_text &lt;- paste(trunc_avatar$full_text, collapse = &quot;&quot;) %&gt;% 
  tolower()</code></pre>
<p>We have 826569 characters in our dataset.</p>
<p><br/></p>
<p>Next, we prepare our input data. The task for learning the language model is, given a sequence of 60 characters from the <em>Avatar</em> script, predict the next character. The following code prepares the <code>(input sequences, next character)</code> pairs and one-hot encodes the characters within those sequences:</p>
<pre class="r"><code>maxlen &lt;- 60  # Length of extracted character sequences

step &lt;- 3  # We sample a new sequence every `step` characters
  
text_indexes &lt;- seq(1, nchar(text) - maxlen, by = step)

# This holds our extracted sequences
sentences &lt;- str_sub(text, text_indexes, text_indexes + maxlen - 1)

# This holds the targets (the follow-up characters)
next_chars &lt;- str_sub(text, text_indexes + maxlen, text_indexes + maxlen)

cat(&quot;Number of sequences: &quot;, length(sentences), &quot;\n&quot;)

# List of unique characters in the corpus
chars &lt;- unique(sort(strsplit(text, &quot;&quot;)[[1]]))
cat(&quot;Unique characters:&quot;, length(chars), &quot;\n&quot;)

# Dictionary mapping unique characters to their index in `chars`
char_indices &lt;- 1:length(chars) 
names(char_indices) &lt;- chars

# Next, one-hot encode the characters into binary arrays.
cat(&quot;Vectorization...\n&quot;) 
x &lt;- array(0L, dim = c(length(sentences), maxlen, length(chars)))
y &lt;- array(0L, dim = c(length(sentences), length(chars)))
for (i in 1:length(sentences)) {
  sentence &lt;- strsplit(sentences[[i]], &quot;&quot;)[[1]]
  for (t in 1:length(sentence)) {
    char &lt;- sentence[[t]]
    x[i, t, char_indices[[char]]] &lt;- 1
  }
  next_char &lt;- next_chars[[i]]
  y[i, char_indices[[next_char]]] &lt;- 1
}</code></pre>
<p>The above code block is ripped straight from <em>Deep Learning in R</em> and is licensed under MIT. You can see the notebook <a href="https://github.com/jjallaire/deep-learning-with-r-notebooks/blob/master/notebooks/8.1-text-generation-with-lstm.Rmd">here</a>.</p>
<p><br/></p>
<p>Next, we create, compile, and fit a model using keras:</p>
<pre class="r"><code>library(keras)

model &lt;- keras_model_sequential() %&gt;% 
  layer_lstm(units = 128, input_shape = c(maxlen, length(chars)), recurrent_activation = &quot;sigmoid&quot;) %&gt;% 
  layer_dense(units = length(chars), activation = &quot;softmax&quot;)

optimizer &lt;- optimizer_adam(lr = .01)

model %&gt;% compile(
  loss = &quot;categorical_crossentropy&quot;, 
  optimizer = optimizer,
  metrics=&#39;accuracy&#39;
)   

model %&gt;% fit(x, y, batch_size = 4096, epochs = 40)</code></pre>
<p>The specifying of <code>recurrent_activation = "sigmoid"</code> is crucial here, as the default recurrent activation function is not compatible with CuDNN kernel, which results in less utilization of the gpu and slower training times.</p>
<p><br/></p>
<p>Rstudio shows a nice graph depicting loss over successive epochs:</p>
<p><img src="loss-and-accuracy.png" /></p>
<p>From here, we write the code for sampling from the language model. Again, much of this is from <em>Deep Learning with R</em>, but I’ve re-factored it to make it more readable. At a high-level, what we’re doing here is grabbing the first 60 characters from the first lines of the show, asking our model to generate a prediction of the next character, and then sampling from a distribution of the predicted characters modified by the <code>temperature</code> parameter. The higher the temperature, the more “creative” the predicted next character will be. Once, we select a character, we repeat this process with the newly added character placed into the input string.</p>
<pre class="r"><code>sample_next_char &lt;- function(preds, temperature = 1.0) {
  preds &lt;- as.numeric(preds)
  preds &lt;- log(preds) / temperature
  exp_preds &lt;- exp(preds)
  preds &lt;- exp_preds / sum(exp_preds)
  which.max(t(rmultinom(1, 1, preds)))
}

update_text_window &lt;- function(generated_text, next_char) {
  paste0(generated_text, next_char) %&gt;% 
    substring(2)
}

one_hot_encode &lt;- function(generated_text) {
  sampled &lt;- array(0, dim = c(1, maxlen, length(chars)))
  generated_chars &lt;- strsplit(generated_text, &quot;&quot;)[[1]]
  for (t in 1:length(generated_chars)) {
    char &lt;- generated_chars[[t]]
    sampled[1, t, char_indices[[char]]] &lt;- 1
  }
  sampled
}

generate_character &lt;- function(generated_text, temperature) {
  sampled &lt;- generated_text %&gt;% one_hot_encode()
  preds &lt;- model %&gt;% predict(sampled, verbose = 0)
  next_index &lt;- sample_next_char(preds[1, ], temperature)
  chars[[next_index]]
}

seed_text &lt;- str_sub(text, 1, 60)

for (temperature in c(0.2, 0.5, 1.0, 1.2)) {
    
    cat(&quot;------ temperature:&quot;, temperature, &quot;\n&quot;)
    cat(paste(&quot;seed text: &quot;, seed_text), &quot;\n&quot;)
    
    text_window &lt;- seed_text
    
    for (i in 1:400) {
      
      next_char &lt;- generate_character(text_window, temperature)
      cat(next_char)
      text_window &lt;- text_window %&gt;% update_text_window(next_char)
      
    }
    cat(&quot;\n\n&quot;)
}</code></pre>
</div>

    </article><section class="article labels"><a class="tag" href=/tags/r/>r</a><a class="tag" href=/tags/keras/>keras</a><a class="tag" href=/tags/nlp/>nlp</a></section></div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/rpost/a-tidy-analysis-of-astronauts-and-their-missions/"><span class="iconfont icon-article"></span>A Tidy Analysis of Astronauts and Their Missions</a></p></section></div>
<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script></section><section id="footer"><div style="display: flex; flex-direction: row; justify-content: center;">
  <p>
    &copy; 2024 Matt Dupree
  </p>
</div>
</section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script><script src="/js/core.min.38b060a751ac96384cd9327eb1b1e36a21fdb71114be07434c0cc7bf63f6e1da274edebfe76f65fbd51ad2f14898b95b.js" integrity="sha384-OLBgp1GsljhM2TJ&#43;sbHjaiH9txEUvgdDTAzHv2P24donTt6/529l&#43;9Ua0vFImLlb"></script></body>

</html>